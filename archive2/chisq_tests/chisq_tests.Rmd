---
title: "Chi-squared test for independence"
output:
  tufte::tufte_handout: default
  tufte::tufte_html: default
date: 'Date: `r format(Sys.time(), "%Y-%m-%d")`'
---

```{r, echo=FALSE, warning=FALSE}
library(knitr)
library(tufte)
load('../brfss_2015_subset.RData')
```

# What it is

```{r, echo=FALSE, fig.margin=TRUE, fig.cap="A chi-squared distribution with 3 degrees of freedom."}
xmx = 10
crit = 6
x = seq(0, xmx, 0.001)
y = dchisq(x, df=3)

par(mar=c(2, 1, 1, 1))

plot(x, y, type='l', xlab='', yaxt='n', ylab='', frame=FALSE)
polygon(x=c(x[x>=crit], xmx, crit), y=c(y[x>=crit], 0, 0), 
        col='lightblue', border='slateblue')
lines(x, y)
abline(h=0, col='grey')
text(x=6, y=0.20, labels=expression(chi[(3)]^2), cex=2)
```

The $\chi^2$ test for independence (mathematically equivalent to the $\chi^2$ test for homogeneity of proportions) investigates the relationship between two categorical variables (nominal or ordinal). The example below compares BMI category between genders. This could be equivalently stated as comparing the distribution of a categorical variable across multiple populations (i.e., how BMI is distributed across genders).


# When to use it

The $\chi^2$ test requires observations to be sampled independently as well as a large sample.  The standard large sample rule of thumb is an expected count of at least 5 in each table cell (see *Why it Works* for details).


# How to use it

As with all tests, begin by stating the hypotheses:

$H_0$: $X$ and $Y$ are independent.  
$H_A$: $X$ and $Y$ are not independent.

Equivalently,

$H_0$: $Y$ has the same distribution for all levels of $X$.  
$H_A$: $Y$ does not have the same distribution for all levels of $X$.

In `R`, create a two-way table using the `table()` function:

```{r}
tab = table(d$sex, d$bmicat)
```

which yields^[To view the margin totals, use `addmargins(tab)`.]

```{r, echo=FALSE}
kable(tab)
```

Then pass the table to the `chisq.test()` function:^[You want to use the initial table, not the table with the margins.]

```{r, eval=FALSE}
chisq.test(tab)
```

The output gives you, in order, the test statistic ($X^2$), degrees of freedom, and p-value. More information is kept hidden and can be accessed by saving the test as a new object.


# Why it works

The $\chi^2$ test for independence is based on the concept of independence from probability theory^[If $A$ and $B$ are independent, then $P(A,B)=P(A)\times P(B)$].

Using `addmargins()`, we get

```{r, echo=FALSE}
kable(addmargins(tab))
```

Take for example, the cell in Row 1, Column 1. By independence (assumed in $H_0$), 

$$P(female, underweight) = P(female)\times P(underweight)$$

Using the magins, each probability can be estimated by data:

$$P(female) \approx \frac{267}{500}\quad \mathrm{and}\quad 
  P(underweight) \approx \frac{12}{500}$$

So, 

$$P(female, underweight) \approx \frac{267}{500} \times \frac{12}{500} = \frac{(267)(12)}{500^2}$$

The expected count for that cell, $E$, is ^[In general, $E = \frac{(row\,total) \times (column\,total)}{n}$]

$$E = 500 \cdot \frac{(267)(12)}{500^2} = 500\cdot \frac{0.013}{500^2} = \frac{0.013}{500}$$

For the $\chi^2$ tests to work well, each $E$ should be at least 5.^[To view the expected counts, use `chisq.test(tab)$expected`.]

The test statistic then is calculated as

$$X^2 = \sum_{cell\,i}{\frac{(O_i - E_i)^2}{E_i}}$$

This is always positive. Additionally, the farther the observed $O$ is from the expected $E$ in each cell, the bigger $X^2$. Therefore, large values of $X^2$ provide evidence against the null hypothesis.

The degrees of freedom affect the shape of the $\chi^2$ distribution and come from the minimum number of cells needed in order to fix the entire table with the given margin totals. For example, in the table above if female underweight, female normal, and female overweight are known, the rest of the values are fixed (male underweight = 12 - female underweight; male normal = 167 - female normal; male overweight = 180 - female overweight; etc.). Thus, $df=3$.
